{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24dde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1f2d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348103</th>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348106</th>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348107</th>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348108</th>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348110</th>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class\n",
       "2       Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "3       Am I weird I don't get affected by compliments...  non-suicide\n",
       "4       Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "8               i need helpjust help me im crying so hard      suicide\n",
       "9       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...      suicide\n",
       "...                                                   ...          ...\n",
       "348103  If you don't like rock then your not going to ...  non-suicide\n",
       "348106  You how you can tell i have so many friends an...  non-suicide\n",
       "348107  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...  non-suicide\n",
       "348108  The usual stuff you find hereI'm not posting t...      suicide\n",
       "348110  I still haven't beaten the first boss in Hollo...  non-suicide\n",
       "\n",
       "[232074 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Suicide_Detection.csv', index_col=\"Unnamed: 0\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb47ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d26a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         Ex Wife Threatening SuicideRecently I left my ...\n",
       "3         Am I weird I don't get affected by compliments...\n",
       "4         Finally 2020 is almost over... So I can never ...\n",
       "8                 i need helpjust help me im crying so hard\n",
       "9         I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...\n",
       "                                ...                        \n",
       "348103    If you don't like rock then your not going to ...\n",
       "348106    You how you can tell i have so many friends an...\n",
       "348107    pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...\n",
       "348108    The usual stuff you find hereI'm not posting t...\n",
       "348110    I still haven't beaten the first boss in Hollo...\n",
       "Name: text, Length: 232074, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.Series(df['text'])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c22c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         1\n",
       "3         0\n",
       "4         0\n",
       "8         1\n",
       "9         1\n",
       "         ..\n",
       "348103    0\n",
       "348106    0\n",
       "348107    0\n",
       "348108    1\n",
       "348110    0\n",
       "Name: suicide, Length: 232074, dtype: uint8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicide = pd.Series(pd.get_dummies(df['class'], drop_first=True)['suicide'])\n",
    "suicide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e8b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  suicide\n",
       "0       Ex Wife Threatening SuicideRecently I left my ...        1\n",
       "1       Am I weird I don't get affected by compliments...        0\n",
       "2       Finally 2020 is almost over... So I can never ...        0\n",
       "3               i need helpjust help me im crying so hard        1\n",
       "4       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...        1\n",
       "...                                                   ...      ...\n",
       "232069  If you don't like rock then your not going to ...        0\n",
       "232070  You how you can tell i have so many friends an...        0\n",
       "232071  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...        0\n",
       "232072  The usual stuff you find hereI'm not posting t...        1\n",
       "232073  I still haven't beaten the first boss in Hollo...        0\n",
       "\n",
       "[232074 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = { 'Text': text, 'suicide': suicide }\n",
    "df = pd.DataFrame(df)\n",
    "df = df.reset_index().drop('index', axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b40e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc38f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174055, 2), (58019, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998c59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7db46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/agoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/agoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/agoyal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/agoyal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62168656",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words(\"english\")\n",
    "punctuations = list(string.punctuation)\n",
    "stop += punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a4d0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bcc8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1])).lower()\n",
    "            output_words.append(clean_word)\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf92726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_text = np.array(train_data[\"Text\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f34cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_text = [clean(word_tokenize(words)) for words in Xtrain_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c6862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_text = [\" \".join(words) for words in Xtrain_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14fb4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features = 2000)\n",
    "temp = count_vec.fit_transform(Xtrain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "177f6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = temp.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf0fbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_text = np.array(test_data[\"Text\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "510847d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_text = [clean(word_tokenize(words)) for words in Xtest_text]\n",
    "Xtest_text = [\" \".join(words) for words in Xtest_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "166b8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = count_vec.transform(Xtest_text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "567c8899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30f306bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_data[\"suicide\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db609a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1dfd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = rf(n_estimators = 80, random_state = 18, max_depth = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c48f079a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=100, n_estimators=80, random_state=18)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_features, train_data['suicide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2c4c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clf.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ce7204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8954308071493821"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_data['suicide'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "841be345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.tree_.max_depth for estimator in clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124d784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neattext in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (0.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install neattext\n",
    "import neattext.functions as nfx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as plx\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "from keras.layers import Embedding,Dense,LSTM,Bidirectional,GlobalMaxPooling1D,Input,Dropout\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a30e7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-macosx_10_11_x86_64.whl (199.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199.0 MB 79 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9 MB 61.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras~=2.6 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65 kB 15.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131 kB 68.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 462 kB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.18.1-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0 MB 34.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 42.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42 kB 11.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.6 MB 80.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57 kB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-macosx_10_10_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.9 MB 30.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.5 MB 37.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 154 kB 101.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 781 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 155 kB 36.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77 kB 28.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/agoyal/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146 kB 35.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=010c23aa6215b27481995251a4a731b24489e9272c866fb024e44ec6a71996ab\n",
      "  Stored in directory: /Users/agoyal/Library/Caches/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=b919ab0606322bcc41cb3cb822e5ab599fb83021c03f4856571bfc5482f519bc\n",
      "  Stored in directory: /Users/agoyal/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.14.1 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.3.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.0 h5py-3.1.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7854c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
